{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and list data-bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, numpy as np, pycrfsuite\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import functions and list data-bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rf_utils import alternative_data_base, extract_features, get_entities, extract_words_from_X\n",
    "dbs_path = f'{module_path}/query_dbs/'\n",
    "models_path = f'{module_path}/models/'\n",
    "list_files = os.listdir(dbs_path)\n",
    "list_query_type = [file_name.replace('.json','') for file_name in list_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a class that load the data-base, train a model, and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFields:\n",
    "    def __init__(self, type_of_query):\n",
    "        self.type_of_query = type_of_query\n",
    "        self.model_file_path = f'{models_path}{self.type_of_query}.model'\n",
    "    \n",
    "    def drop_duplicates(self):\n",
    "        temp = [json.dumps(liste) for liste in self.alternative_data_with_duplicates]\n",
    "        temp = list(set(temp))\n",
    "        temp = [json.loads(liste) for liste in temp]\n",
    "        self.alternative_data = temp\n",
    "        \n",
    "    def load_db(self,index):\n",
    "        data = json.load(open(dbs_path  + self.type_of_query+ '.json'))[self.type_of_query]\n",
    "        self.alternative_data_with_duplicates = alternative_data_base(data)\n",
    "        self.drop_duplicates()\n",
    "        X = [extract_features(query) for query in self.alternative_data]\n",
    "        y = [get_entities(query) for query in self.alternative_data]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2)\n",
    "        if index == 0:\n",
    "            print(f'For the type of query : {type_of_query} \\n')\n",
    "            print(\"Raw data looks like : \")\n",
    "            pprint(data[0])\n",
    "            print(f\"\\n Alternative data looks like :\")\n",
    "            pprint(self.alternative_data[0])\n",
    "            print('\\n Data to feed the algorithm looks like : ')\n",
    "            pprint(X[0])\n",
    "            \n",
    "    def train_model(self,l1_penalty=0.1,l2_penalty=0.01,max_iterations=200):\n",
    "        trainer = pycrfsuite.Trainer(verbose=True)\n",
    "        for xseq, yseq in zip(self.X_train, self.y_train):\n",
    "            trainer.append(xseq, yseq)\n",
    "        trainer.set_params({'c1': l1_penalty,\n",
    "                            'c2': l2_penalty,\n",
    "                            'max_iterations': max_iterations\n",
    "                            })\n",
    "        trainer.train(self.model_file_path)\n",
    "    def pred(self):\n",
    "        tagger = pycrfsuite.Tagger()\n",
    "        tagger.open(self.model_file_path)\n",
    "        self.y_pred = [tagger.tag(xseq) for xseq in self.X_test] \n",
    "        self.correspondance = [(extract_words_from_X(xseq),yseq,tagger.tag(xseq)) for xseq, yseq in zip(self.X_test,self.y_test)]\n",
    "        \n",
    "    def test_sample(self):\n",
    "        self.pred()\n",
    "        i = 23\n",
    "        for x, y in zip(self.y_pred[i], [x[1].split(\"=\")[1] for x in self.X_test[i]]):\n",
    "            print(\"%s (%s)\" % (y, x))\n",
    "    \n",
    "    def accuracy_report(self):\n",
    "        self.pred()\n",
    "        # Convert the sequences of tags into a 1-dimensional array\n",
    "        self.predictions = np.array([tag for row in self.y_pred for tag in row])\n",
    "        self.truths = np.array([tag for row in self.y_test for tag in row])\n",
    "        print(classification_report(\n",
    "            self.truths, self.predictions\n",
    "             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save a model for each type of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the type of query : PlayMusic \n",
      "\n",
      "Raw data looks like : \n",
      "{'data': [{'text': 'I need to hear the '},\n",
      "          {'entity': 'music_item', 'text': 'song'},\n",
      "          {'text': ' '},\n",
      "          {'entity': 'track', 'text': 'Aspro Mavro'},\n",
      "          {'text': ' from '},\n",
      "          {'entity': 'artist', 'text': 'Bill Szymczyk'},\n",
      "          {'text': ' on '},\n",
      "          {'entity': 'service', 'text': 'Youtube'}]}\n",
      "\n",
      " Alternative data looks like :\n",
      "[{'entity': 'None', 'text': 'Play'},\n",
      " {'entity': 'None', 'text': 'the'},\n",
      " {'entity': 'sort', 'text': 'greatest'},\n",
      " {'entity': 'music_item', 'text': 'soundtrack'},\n",
      " {'entity': 'None', 'text': 'by'},\n",
      " {'entity': 'artist', 'text': 'Nhat'},\n",
      " {'entity': 'artist', 'text': 'Son'},\n",
      " {'entity': 'None', 'text': 'on'},\n",
      " {'entity': 'service', 'text': 'Last'},\n",
      " {'entity': 'service', 'text': 'Fm'}]\n",
      "\n",
      " Data to feed the algorithm looks like : \n",
      "[['bias',\n",
      "  'word.lower=play',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=True',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=0',\n",
      "  'len_query=10',\n",
      "  'BOS',\n",
      "  '+1:word.lower=the',\n",
      "  '+1:word.istitle=False',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=the',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=False',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=1',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=play',\n",
      "  '-1:word.istitle=True',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=greatest',\n",
      "  '+1:word.istitle=False',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=greatest',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=False',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=2',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=the',\n",
      "  '-1:word.istitle=False',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=soundtrack',\n",
      "  '+1:word.istitle=False',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=soundtrack',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=False',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=3',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=greatest',\n",
      "  '-1:word.istitle=False',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=by',\n",
      "  '+1:word.istitle=False',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=by',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=False',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=4',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=soundtrack',\n",
      "  '-1:word.istitle=False',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=nhat',\n",
      "  '+1:word.istitle=True',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=nhat',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=True',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=5',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=by',\n",
      "  '-1:word.istitle=False',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=son',\n",
      "  '+1:word.istitle=True',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=son',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=True',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=6',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=nhat',\n",
      "  '-1:word.istitle=True',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=on',\n",
      "  '+1:word.istitle=False',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=on',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=False',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=7',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=son',\n",
      "  '-1:word.istitle=True',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=last',\n",
      "  '+1:word.istitle=True',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=last',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=True',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=8',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=on',\n",
      "  '-1:word.istitle=False',\n",
      "  '-1:word.isdigit=False',\n",
      "  '+1:word.lower=fm',\n",
      "  '+1:word.istitle=True',\n",
      "  '+1:word.isdigit=False'],\n",
      " ['bias',\n",
      "  'word.lower=fm',\n",
      "  'word.isupper=False',\n",
      "  'word.istitle=True',\n",
      "  'word.isdigit=False',\n",
      "  'place_in_query=9',\n",
      "  'len_query=10',\n",
      "  '-1:word.lower=last',\n",
      "  '-1:word.istitle=True',\n",
      "  '-1:word.isdigit=False']]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       None       1.00      1.00      1.00      1505\n",
      "      album       1.00      0.87      0.93       119\n",
      "     artist       0.99      1.00      1.00       492\n",
      "      genre       0.92      1.00      0.96        34\n",
      " music_item       0.99      1.00      1.00       176\n",
      "   playlist       0.92      0.98      0.95        62\n",
      "    service       1.00      1.00      1.00       188\n",
      "       sort       0.99      1.00      0.99        76\n",
      "      track       0.94      0.99      0.96       121\n",
      "       year       1.00      0.99      1.00       142\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2915\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                  None       1.00      1.00      1.00      2822\n",
      "                  city       0.96      1.00      0.98       151\n",
      "               country       0.96      0.99      0.98       107\n",
      "               cuisine       0.98      0.98      0.98        44\n",
      "              facility       1.00      1.00      1.00        43\n",
      "party_size_description       0.98      1.00      0.99       248\n",
      "     party_size_number       1.00      1.00      1.00       209\n",
      "                   poi       0.99      0.90      0.94        78\n",
      "       restaurant_name       1.00      0.99      1.00       160\n",
      "       restaurant_type       1.00      1.00      1.00       320\n",
      "           served_dish       0.96      0.96      0.96        91\n",
      "                  sort       1.00      1.00      1.00        55\n",
      "      spatial_relation       1.00      1.00      1.00       129\n",
      "                 state       1.00      1.00      1.00       130\n",
      "             timeRange       0.99      1.00      1.00       352\n",
      "\n",
      "           avg / total       0.99      0.99      0.99      4939\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/francois/Documents/Python_files/NLP/slot_filling/query_dbs/.DS_Store.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-03efd738d2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_of_query\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_query_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrandom_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_of_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrandom_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#random_fields.train_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrandom_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8b49f6ed488f>\u001b[0m in \u001b[0;36mload_db\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbs_path\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_of_query\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_of_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malternative_data_with_duplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malternative_data_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/francois/Documents/Python_files/NLP/slot_filling/query_dbs/.DS_Store.json'"
     ]
    }
   ],
   "source": [
    "for index, type_of_query in enumerate(list_query_type):\n",
    "    random_fields = RandomFields(type_of_query)\n",
    "    random_fields.load_db(index=index)\n",
    "    #random_fields.train_model()\n",
    "    random_fields.accuracy_report()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fields = RandomFields(\"AllQueries\")\n",
    "random_fields.load_db(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                      None       1.00      1.00      1.00     12697\n",
      "                     album       0.90      0.92      0.91       124\n",
      "                    artist       0.99      0.97      0.98       836\n",
      "               best_rating       1.00      1.00      1.00       202\n",
      "                      city       0.98      0.98      0.98       381\n",
      "     condition_description       1.00      1.00      1.00        86\n",
      "     condition_temperature       1.00      1.00      1.00       108\n",
      "                   country       1.00      0.99      0.99       250\n",
      "                   cuisine       0.98      0.93      0.95        44\n",
      "          current_location       1.00      1.00      1.00        76\n",
      "               entity_name       0.93      0.96      0.94       385\n",
      "                  facility       1.00      0.97      0.99        40\n",
      "                     genre       0.93      0.96      0.95        28\n",
      "            geographic_poi       0.98      1.00      0.99       280\n",
      "             location_name       1.00      1.00      1.00       301\n",
      "                movie_name       0.96      0.95      0.95       665\n",
      "                movie_type       1.00      1.00      1.00       168\n",
      "                music_item       1.00      0.99      0.99       387\n",
      "      object_location_type       1.00      1.00      1.00       170\n",
      "               object_name       0.98      0.98      0.98      2134\n",
      "object_part_of_series_type       1.00      0.99      0.99        76\n",
      "             object_select       0.98      0.99      0.98       207\n",
      "               object_type       0.99      0.99      0.99       830\n",
      "    party_size_description       1.00      1.00      1.00       284\n",
      "         party_size_number       0.99      0.98      0.98       207\n",
      "                  playlist       0.98      0.99      0.98      1240\n",
      "            playlist_owner       1.00      0.99      0.99       259\n",
      "                       poi       1.00      0.92      0.96        36\n",
      "               rating_unit       1.00      1.00      1.00       229\n",
      "              rating_value       0.99      1.00      0.99       383\n",
      "           restaurant_name       1.00      0.97      0.99       215\n",
      "           restaurant_type       0.99      1.00      1.00       321\n",
      "               served_dish       0.97      0.99      0.98        78\n",
      "                   service       1.00      0.99      1.00       188\n",
      "                      sort       0.99      1.00      1.00       145\n",
      "          spatial_relation       0.99      1.00      0.99       381\n",
      "                     state       1.00      1.00      1.00       257\n",
      "                 timeRange       0.99      0.98      0.99      1107\n",
      "                     track       0.93      0.88      0.91       147\n",
      "                      year       0.99      1.00      1.00       120\n",
      "\n",
      "               avg / total       0.99      0.99      0.99     26072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_fields.accuracy_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etudes des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_fields.correspondance[0]\n",
    "future_df = []\n",
    "for query_id, query in enumerate(random_fields.correspondance):\n",
    "    sentence = query[0]\n",
    "    truths = query[1]\n",
    "    predictions = query[2]\n",
    "    for word, truth_entity, entity_pred in zip(sentence,truths, predictions):\n",
    "        row={}\n",
    "        row ={\"query_id\":query_id, \"word\":word, \"truth_entity\":truth_entity,\"entity_pred\":entity_pred}\n",
    "        future_df.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame(future_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"algo_is_not_right\"] = new.apply(lambda row : row[\"truth_entity\"]!=row[\"entity_pred\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_error_in_query\n",
       "0.0     2736\n",
       "1.0       65\n",
       "2.0       22\n",
       "3.0       19\n",
       "4.0        6\n",
       "5.0        5\n",
       "6.0        4\n",
       "8.0        1\n",
       "16.0       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.groupby('query_id')[\"algo_is_not_right\"].sum().reset_index(name=\"nb_error_in_query\").groupby('nb_error_in_query').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Combinaison des deux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open(dbs_path  + 'AllQueriesNoDuplicates.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
