{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration : underline\">Import packages and datas<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "dbs_path = f'{module_path}/query_dbs/'\n",
    "models_path = f'{module_path}/models/'\n",
    "graphs_path =  f'{module_path}/graphs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classif_utils import flatten, multi_label\n",
    "list_files = os.listdir(dbs_path)\n",
    "list_query_type = [file_name.replace('.json','') for file_name in list_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for index, query_type in enumerate(list_query_type):\n",
    "    if index == 3:\n",
    "        pre_data = json.load(open(dbs_path + query_type + '.json', encoding = 'latin-1'))\n",
    "    else:\n",
    "        pre_data = json.load(open(dbs_path +query_type + '.json'))\n",
    "    datasets.append(pre_data)\n",
    "\n",
    "new = {list(query_type_dico.keys())[0] : list(query_type_dico.values())[0]  for query_type_dico in datasets}\n",
    "new = [ {'query_type' : query_type, 'text_full' : flatten(dict_data)} for query_type, data_query_type in new.items()\n",
    "       for dict_data in data_query_type  ]\n",
    "data_label = pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {query_type : index for index, query_type in enumerate(list_query_type)}\n",
    "data_label['label'] = data_label['query_type'].map(queries) \n",
    "data_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label[\"label_multi\"] = np.nan\n",
    "data_label[\"label_multi\"] = data_label[\"label\"].apply(multi_label)\n",
    "data_label.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,SpatialDropout1D,LSTM,Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_max = 50000\n",
    "max_seq_length = 40\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=nb_words_max, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data_label['text_full'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data_label['text_full'].values)\n",
    "X = pad_sequences(X, maxlen=max_seq_length)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "Y = np.vstack(data_label['label_multi'].values)\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tokenizer\n",
    "with open(f'{models_path}tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_words_max, embedding_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(10, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.0005), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=10, batch_size= 16, validation_data = (X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{models_path}lstm_classif_type.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history[\"loss\"], label = \"Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Validation loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Cross entropy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history[\"acc\"], label = \"Accuracy\")\n",
    "plt.plot(history.history[\"val_acc\"], label = \"Validation accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{graphs_path}loss_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from random import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "n = len(X_test)\n",
    "\n",
    "lab_pred = np.zeros(n)\n",
    "lab_true = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    lab_pred[i] = np.argmax(pred[i])\n",
    "    lab_true[i] = np.argmax(Y_test[i])\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(lab_true,lab_pred), classes = np.arange(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_lab_to_query_type(lab):\n",
    "    return list_of_query_type[np.argmax(lab)]\n",
    "\n",
    "multi_lab_to_query_type_vect = np.vectorize(multi_lab_to_query_type)\n",
    "\n",
    "def predict(query_batch):\n",
    "    # loading tokenizer\n",
    "    with open(f\"{models_path}tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    X = tokenizer.texts_to_sequences(query_batch)\n",
    "    X = pad_sequences(X, maxlen = 40)\n",
    "    \n",
    "    model = load_model(f\"{models_path}lstm_classif_type.h5')\n",
    "    pred = model.predict(X)\n",
    "    print(pred)\n",
    "    \n",
    "    n = len(X)\n",
    "    lab_pred = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        lab_pred[i] = np.argmax(pred[i])\n",
    "    \n",
    "    return lab_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"Francis petite frappe\", \"I want to listen to techno music\", \"Rate Harry Potter movie\", \n",
    "        \"What will the weather be like tomorow mother fucker ?\", 'I want to book a table for six']\n",
    "test = np.array(test)\n",
    "predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_query_type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
